from ultralytics import YOLO
import torch
import os
import torch.nn as nn
from collections import defaultdict
from ultralytics.nn.modules import Conv, Bottleneck, Concat
from ultralytics.nn.extra_modules.block import EMSConvP, C3k2_EMBC, C3k2_EMSCP


class ChannelAwarePruner:
    def __init__(self, model_path, prune_factor=0.5):
        self.model = YOLO(model_path).model
        self.prune_factor = prune_factor
        self.channel_map = {}
        self.dependency_graph = defaultdict(list)
        self.concat_sources = {}
        self.protected_layers = {
            'model.0.conv': 3,  # å¼ºåˆ¶ä¿ç•™è¾“å…¥é€šé“
            'model.2': 64,
            'model.4': 128,
            'model.7': 256,
            'model.14': 512
        }
        self.min_channels = 16  # å…¨å±€æœ€å°é€šé“æ•°

        self._parse_model_config()
        self._build_full_dependency()
        self._init_channel_records()

    def _parse_model_config(self):
        """å¢å¼ºå‹æ¨¡å‹ç»“æ„è§£æ"""
        try:
            for i, layer in enumerate(self.model.model):
                if isinstance(layer, Concat):
                    sources = getattr(layer, 'from_idx', [])
                    if not sources and i < len(self.model.yaml):
                        args = self.model.yaml[i][1]
                        sources = args[0] if isinstance(args, list) else []
                    self.concat_sources[layer] = [i + x if x < 0 else x for x in sources]
        except Exception as e:
            print(f"âš ï¸ é…ç½®è§£æå¼‚å¸¸: {str(e)}")

    def _build_full_dependency(self):
        """ä¸‰ç»´ä¾èµ–å…³ç³»æ„å»º"""
        parent_map = defaultdict(list)
        for name, module in self.model.named_modules():
            for child_name, child in module.named_children():
                parent_map[child].append(module)
                self.dependency_graph[module].append(child)

        # æ„å»ºé€†å‘ä¾èµ–
        for concat_layer, sources in self.concat_sources.items():
            for src_idx in sources:
                if src_idx < len(self.model.model):
                    src_module = self.model.model[src_idx]
                    self.dependency_graph[src_module].append(concat_layer)

    def _init_channel_records(self):
        """é€šé“è®°å½•ç³»ç»Ÿåˆå§‹åŒ–"""
        for name, module in self.model.named_modules():
            if isinstance(module, (nn.Conv2d, Concat)):
                self.channel_map[name] = self._get_valid_channels(module)

    def _get_valid_channels(self, module):
        """å®‰å…¨è·å–é€šé“æ•°"""
        if isinstance(module, nn.Conv2d):
            return module.out_channels
        if isinstance(module, Concat):
            return sum(self.channel_map.get(name, 0) for name in self._get_concat_source_names(module))
        return 0

    def _get_concat_source_names(self, concat_layer):
        """è·å–Concatå±‚è¾“å…¥æºåç§°"""
        sources = []
        for idx in self.concat_sources.get(concat_layer, []):
            if idx < len(self.model.model):
                src_module = self.model.model[idx]
                sources.append(self._get_layer_name(src_module))
        return sources

    def auto_prune(self, save_path):
        """å…¨è‡ªåŠ¨å‰ªææµç¨‹"""
        try:
            self._calculate_threshold()
            self._prune_all_modules()
            self._propagate_all_changes()
            self._final_consistency_check()
            self._save_model(save_path)
        except Exception as e:
            print(f"âŒ å‰ªæå¤±è´¥: {str(e)}")
            self._generate_diagnosis()

    def _calculate_threshold(self):
        """åŠ¨æ€é˜ˆå€¼è®¡ç®—"""
        bn_weights = []
        for m in self.model.modules():
            if isinstance(m, nn.BatchNorm2d):
                bn_weights.append(m.weight.abs().detach())
        sorted_weights = torch.sort(torch.cat(bn_weights), descending=True)[0]
        self.threshold = sorted_weights[int(len(sorted_weights) * self.prune_factor)]

    def _prune_all_modules(self):
        """å…¨å±€å‰ªæå…¥å£"""
        for name, module in list(self.model.named_modules()):
            if isinstance(module, (Bottleneck, C3k2_EMBC, C3k2_EMSCP)):
                self._prune_complex_module(module, name)
            elif isinstance(module, nn.Conv2d):
                self._prune_single_conv(module, name)

    def _prune_complex_module(self, module, name):
        """æ™ºèƒ½å¤„ç†å¤æ‚æ¨¡å—"""
        if 'Bottleneck' in str(type(module)):
            print(f"ğŸ”§ å¤„ç†ç“¶é¢ˆæ¨¡å—: {name}")
            for child_name, child in module.named_children():
                if 'act' not in child_name and 'bn' not in child_name:
                    self._prune_single_conv(child, f"{name}.{child_name}")
            return

        print(f"ğŸ› ï¸ å¤„ç†å¤æ‚æ¨¡å—: {name}")
        for child_name, child in module.named_children():
            if isinstance(child, (nn.Conv2d, Bottleneck)):
                self._prune_single_conv(child, f"{name}.{child_name}")
            else:
                self._prune_complex_module(child, f"{name}.{child_name}")

    def _prune_single_conv(self, conv, name):
        """å®‰å…¨å‰ªæé€»è¾‘"""
        if name in self.protected_layers:
            new_channels = self.protected_layers[name]
            print(f"ğŸ›¡ï¸ ä¿æŠ¤å±‚é‡ç½®é€šé“ [{name}] â†’ {new_channels}")
            conv.out_channels = new_channels
            self.channel_map[name] = new_channels
            return

        if not hasattr(conv, 'weight'):
            print(f"â© è·³è¿‡æ— æƒé‡æ¨¡å—: {name}")
            return

        # æ·±åº¦æœç´¢BNå±‚ï¼ˆæ”¯æŒ3å±‚åµŒå¥—ï¼‰
        bn_layers = []

        def _deep_find_bn(m, depth=0):
            if depth > 3: return
            if isinstance(m, nn.BatchNorm2d): bn_layers.append(m)
            for child in m.children(): _deep_find_bn(child, depth + 1)

        _deep_find_bn(conv)

        if bn_layers:
            self._prune_with_bn(conv, name, bn_layers)
        else:
            self._prune_by_weight(conv, name)

    def _prune_with_bn(self, conv, name, bn_layers):
        """æ ‡å‡†BNå‰ªæ"""
        main_bn = bn_layers[0]
        bn_weights = main_bn.weight.abs().detach()

        min_channels = max(self.min_channels, self.protected_layers.get(name, 0))
        keep_idx = torch.where(bn_weights >= self.threshold)[0]

        if len(keep_idx) < min_channels:
            keep_idx = torch.argsort(bn_weights, descending=True)[:min_channels]
        keep_idx = keep_idx[keep_idx < bn_weights.size(0)]

        # æ‰§è¡Œå‰ªæ
        conv.out_channels = len(keep_idx)
        conv.weight = nn.Parameter(conv.weight[keep_idx])
        for bn in bn_layers:
            self._update_bn_params(bn, keep_idx)

        self.channel_map[name] = len(keep_idx)
        print(f"âœ‚ï¸ å‰ªæå®Œæˆ [{name}] ä¿ç•™é€šé“: {len(keep_idx)}")

    def _prune_by_weight(self, conv, name):
        """æƒé‡å‰ªæå¤‡ç”¨æ–¹æ¡ˆ"""
        weight_importance = torch.mean(conv.weight.abs(), dim=(1, 2, 3))
        sorted_idx = torch.argsort(weight_importance, descending=True)

        min_channels = max(self.min_channels, self.protected_layers.get(name, 0))
        keep_idx = sorted_idx[:max(int(len(sorted_idx) * 0.5), min_channels)]

        conv.out_channels = len(keep_idx)
        conv.weight = nn.Parameter(conv.weight[keep_idx])
        self.channel_map[name] = len(keep_idx)
        print(f"âš–ï¸ æƒé‡å‰ªæå®Œæˆ [{name}] ä¿ç•™é€šé“: {len(keep_idx)}")

    def _update_bn_params(self, bn, indices):
        """å®‰å…¨æ›´æ–°BNå‚æ•°"""
        valid_indices = indices[indices < bn.num_features]
        if len(valid_indices) == 0:
            valid_indices = torch.arange(bn.num_features)[:self.min_channels]

        bn.num_features = len(valid_indices)
        bn.weight = nn.Parameter(bn.weight.data[valid_indices])
        bn.bias = nn.Parameter(bn.bias.data[valid_indices])
        bn.running_mean = bn.running_mean[valid_indices]
        bn.running_var = bn.running_var[valid_indices]

    def _propagate_all_changes(self):
        """å…¨å±€é€šé“ä¼ æ’­"""
        print("\nğŸŒ å¼€å§‹é€šé“ä¼ æ’­")
        visited = set()
        for name in list(self.channel_map.keys()):
            module = self._get_module_by_name(name)
            if module and module not in visited:
                self._propagate_changes(module, visited)

    def _propagate_changes(self, module, visited):
        """é€’å½’ä¼ æ’­å˜æ›´"""
        if module in visited: return
        visited.add(module)

        if isinstance(module, nn.Conv2d):
            self._update_dependent_layers(module)

        for dependent in self.dependency_graph.get(module, []):
            self._propagate_changes(dependent, visited)

    def _update_dependent_layers(self, conv):
        """æ›´æ–°ä¾èµ–è¯¥å·ç§¯çš„å±‚"""
        current_out = max(self.channel_map.get(self._get_layer_name(conv), conv.out_channels), self.min_channels)
        print(f"ğŸ”§ ä¼ æ’­é€šé“ [{self._get_layer_name(conv)}] â†’ {current_out}")

        for dependent in self.dependency_graph.get(conv, []):
            if isinstance(dependent, nn.Conv2d):
                self._update_conv_input(dependent, current_out)
            elif isinstance(dependent, Concat):
                self._update_concat_layer(dependent)

    def _update_conv_input(self, conv, target_in):
        """å®‰å…¨æ›´æ–°è¾“å…¥é€šé“"""
        conv_name = self._get_layer_name(conv)
        target_in = max(target_in, self.min_channels)

        if conv.in_channels == target_in:
            return

        print(f"ğŸ”„ æ›´æ–°è¾“å…¥é€šé“ [{conv_name}] {conv.in_channels}â†’{target_in}")

        # å¤„ç†åˆ†ç»„å·ç§¯
        groups = conv.groups
        if groups > 1:
            target_in = target_in * groups

        # æ™ºèƒ½æƒé‡è¿ç§»
        new_weight = torch.zeros(conv.out_channels, target_in, *conv.weight.shape[2:],
                                 device=conv.weight.device)
        min_c = min(target_in, conv.weight.shape[1])
        new_weight[:, :min_c] = conv.weight[:, :min_c]

        conv.weight = nn.Parameter(new_weight)
        conv.in_channels = target_in
        self.channel_map[conv_name] = conv.out_channels

    def _update_concat_layer(self, concat_layer):
        """åŠ¨æ€æ›´æ–°Concatå±‚"""
        concat_name = self._get_layer_name(concat_layer)
        total_channels = sum(
            self.channel_map.get(src_name, 0)
            for src_name in self._get_concat_source_names(concat_layer)
        )
        total_channels = max(total_channels, self.min_channels)

        concat_layer.out_channels = total_channels
        self.channel_map[concat_name] = total_channels
        print(f"ğŸ”— æ›´æ–°Concatå±‚ [{concat_name}] æ€»é€šé“: {total_channels}")

    def _final_consistency_check(self):
        """æœ€ç»ˆä¸€è‡´æ€§éªŒè¯"""
        print("\nğŸ” æ‰§è¡Œæœ€ç»ˆéªŒè¯")
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Conv2d):
                # è¾“å…¥é€šé“éªŒè¯
                expected_in = self._get_expected_input(name)
                expected_in = max(expected_in, self.min_channels)

                if module.in_channels != expected_in:
                    print(f"âš¡ ä¿®å¤è¾“å…¥é€šé“ [{name}] {module.in_channels}â†’{expected_in}")
                    module.in_channels = expected_in

                # è¾“å‡ºé€šé“éªŒè¯
                expected_out = max(self.channel_map.get(name, module.out_channels), self.min_channels)
                if module.out_channels != expected_out:
                    print(f"âš¡ ä¿®å¤è¾“å‡ºé€šé“ [{name}] {module.out_channels}â†’{expected_out}")
                    module.out_channels = expected_out

    def _get_expected_input(self, name):
        """æ™ºèƒ½è·å–é¢„æœŸè¾“å…¥é€šé“"""
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ä¸Šæ¸¸å±‚
        for parent, children in self.dependency_graph.items():
            parent_name = self._get_layer_name(parent)
            if any(self._get_layer_name(child) == name for child in children):
                return self.channel_map.get(parent_name, 0)
        return 0

    def _get_layer_name(self, module):
        """è·å–æ¨¡å—çš„å®Œæ•´åç§°"""
        for name, m in self.model.named_modules():
            if m is module:
                return name
        return "unknown"

    def _save_model(self, save_path):
        """æ¨¡å‹ä¿å­˜ä¸éªŒè¯"""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        torch.save(self.model.state_dict(), save_path)

        try:
            test_input = torch.randn(1, 3, 640, 640)
            self.model(test_input)
            print(f"âœ… éªŒè¯æˆåŠŸï¼æ¨¡å‹ä¿å­˜è‡³: {save_path}")
        except Exception as e:
            print(f"âŒ éªŒè¯å¤±è´¥: {str(e)}")
            self._generate_diagnosis()

    def _generate_diagnosis(self):
        """ç”Ÿæˆæ·±åº¦è¯Šæ–­æŠ¥å‘Š"""
        print("\nğŸ” æ·±åº¦è¯Šæ–­æŠ¥å‘Š:")
        # æ£€æŸ¥æ‰€æœ‰å·ç§¯å±‚
        conv_issues = []
        for name, module in self.model.named_modules():
            if isinstance(module, nn.Conv2d):
                try:
                    actual_in = module.weight.shape[1] * module.groups
                    if actual_in != module.in_channels:
                        conv_issues.append(f"{name.ljust(30)} è¾“å…¥: {module.in_channels}â‰ {actual_in}")
                except:
                    pass

        print(f"å·ç§¯å±‚å¼‚å¸¸ ({len(conv_issues)} å¤„):")
        for issue in conv_issues[:5]:
            print(issue)


if __name__ == "__main__":
    pruner = ChannelAwarePruner(
        model_path=r'D:/BaiduNetdiskDownload/yolo11train/yolo11é­”é¬¼é¢å…·æœ€æ–°ç‰ˆ/ultralytics-yolo11-main/runs/train/exp111/weights/best.pt',
        prune_factor=0.5
    )
    pruner.auto_prune(r'pruned_model.pt')